<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016 Qualcomm Technologies, Inc. All rights reserved. 
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 08/23/16   LB   Created to support generation of SNPE UG and SDK HTML document.
 /dox/header.html 
 This template file includes information similar to /dox/header.tex, but is 
 used to generate the /dox/html/index.html page, which compiles all the 
 html-generated files to produce the html document. This file is called in the 
 /dox/Doxy_Config.ldd file.
 header.html - Header file used to customize the headers on each HTML page page.-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Tools</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tools.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Tools </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This chapter describes the various SDK tools and features. </p><ul>
<li>
<a class="el" href="tools.html#tools_snpe-net-run">snpe-net-run</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-bench">snpe_bench.py</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-caffe-to-dlc">snpe-caffe-to-dlc</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-caffe2-to-dlc">snpe-caffe2-to-dlc</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-diagview">snpe-diagview</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-dlc-info">snpe-dlc-info</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a>  </li>
<li>
<a class="el" href="tools.html#tools_snpe-onnx-to-dlc">snpe-onnx-to-dlc</a>  </li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-net-run"></a>
snpe-net-run</h1>
<p>snpe-net-run loads a DLC file, loads the data for the input tensor(s), and executes the network on the specified runtime.</p>
<pre class="fragment">DESCRIPTION:
------------
Example application demonstrating how to load and execute a neural network
using the SNPE C++ API.


REQUIRED ARGUMENTS:
-------------------
  --container  &lt;FILE&gt;   Path to the DL container containing the network.
  --input_list &lt;FILE&gt;   Path to a file listing the inputs for the network.


OPTIONAL ARGUMENTS:
-------------------
  --use_gpu             Use the GPU runtime for SNPE.
  --use_dsp             Use the DSP fixed point runtime for SNPE.
  --debug               Specifies that output from all layers of the network
                        will be saved.
  --output_dir &lt;DIR&gt;    The directory to save output to. Defaults to ./output
  --storage_dir &lt;DIR&gt;   The directory to store SNPE metadata files
  --encoding_type &lt;VAL&gt; Specifies the encoding type of input file. Valid settings are "nv21".
                        Cannot be combined with --userbuffer*.
  --userbuffer_float    [EXPERIMENTAL] Specifies to use userbuffer for inference, and the input type is float.
                        Cannot be combined with --encoding_type.
  --userbuffer_tf8      [EXPERIMENTAL] Specifies to use userbuffer for inference, and the input type is tf8exact0.
                        Cannot be combined with --encoding_type.
  --perf_profile &lt;VAL&gt;  Specifies perf profile to set. Valid settings are "balanced" , "default", "high_performance",
                        "sustained_high_performance" , "power_saver" and "system_settings".
                        NOTE: "balanced" and "default" are the same.  "default" is being deprecated in the future.
  --enable_cpu_fallback Enables cpu fallback functionality. Defaults to disable mode.
  --input_name &lt;INPUT_NAME&gt; Specifies the name of input for which dimensions are specified.

  --input_dimensions &lt;INPUT_DIM&gt;  Specifies new dimensions for input whose name is specified in input_name. e.g. "1,224,224,3". 
                        For multiple inputs, specify --input_name and --input_dimensions multiple times.
  --gpu_mode &lt;VAL&gt;      Specifies gpu operation mode. Valid settings are "default", "float16".
                        default = float32 math and float16 storage (equiv. use_gpu arg).
                        float16 = float16 math and float16 storage.
  --help                 Show this help message.
  --version              Show SNPE Version Number.</pre><p>This binary outputs raw output tensors into the output folder by default. Examples of using snpe-net-run can be found in <a class="el" href="tutorial_alexnet.html">Running AlexNet</a> tutorial.<br />
 <br />
 Additional details:<br />
 </p><ul>
<li>
<em>input_list argument:</em><a class="anchor" id="tools_snpe-net-run_input_list"></a> <ul>
<li>
<p class="startli">snpe-net-run can take multiple input files as input data per iteration, and specify multiple output names, in an input list file formated as below: </p><pre class="fragment">      #&lt;output_name&gt;[&lt;space&gt;&lt;output_name&gt;]
      &lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
      â€¦</pre><p> The first line starting with a "#" specifies the output layers' names. If there is more than one output, a whitespace should be used as a delimiter. Following the first line, you can use multiple lines to supply input files, one line per iteration, and each line only supply one layer.If there is more than one input per line, a whitespace should be used as a delimiter.</p>
<p class="endli">Here is an example, where the layer names are "Input_1" and "Input_2", and inputs are located in the path "Placeholder_1/real_input_inputs_1/". Its input list file should look like this: </p><pre class="fragment">      #Output_1 Output_2
      Input_1:=Placeholder_1/real_input_inputs_1/0-0#e6fb51.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/0-1#8a171b.rawtensor
      Input_1:=Placeholder_1/real_input_inputs_1/1-0#67c965.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/1-1#54f1ff.rawtensor
      Input_1:=Placeholder_1/real_input_inputs_1/2-0#b42dc6.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/2-1#346a0e.rawtensor</pre>  </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-bench"></a>
snpe_bench.py</h1>
<p>python script snpe_bench.py runs a DLC neural network and collects benchmark performance information.</p>
<pre class="fragment">usage: snpe_bench.py [-h] -c CONFIG_FILE [-o OUTPUT_BASE_DIR_OVERRIDE]
                     [-v DEVICE_ID_OVERRIDE] [-a] [-t DEVICE_OS_TYPE_OVERRIDE]
                     [-d] [-s SLEEP] [-b USERBUFFER_MODE] [-p PERFPROFILE]

Run the snpe_bench

required arguments:
  -c CONFIG_FILE, --config_file CONFIG_FILE
                        Path to a valid config file

optional arguments:
  -o OUTPUT_BASE_DIR_OVERRIDE, --output_base_dir_override OUTPUT_BASE_DIR_OVERRIDE
                        Sets the output base directory.
  -v DEVICE_ID_OVERRIDE, --device_id_override DEVICE_ID_OVERRIDE
                        Use this device ID instead of the one supplied in
                        config file. Cannot be used with -a
  -a, --run_on_all_connected_devices_override
                        Runs on all connected devices, currently only support
                        1. Cannot be used with -v
  -t DEVICE_OS_TYPE_OVERRIDE, --device_os_type_override DEVICE_OS_TYPE_OVERRIDE
                        Specify the target OS type, valid options are
                        ['android', 'android-arm32-llvm', 'android-aarch64',
                        'android-aarch64-llvm', 'le', 'le64_gcc4.9',
                        'le_gcc4.8hf']
  -d, --debug           Set to turn on debug log
  -s SLEEP, --sleep SLEEP
                        Set number of seconds to sleep between runs e.g. 20
                        seconds
  -b USERBUFFER_MODE, --userbuffer_mode USERBUFFER_MODE
                        [EXPERIMENTAL] Enable user buffer mode, default to
                        float, can be tf8exact0
  -p PERFPROFILE, --perfprofile PERFPROFILE
                        Set the benchmark operating mode (balanced, default,
                        sustained_high_performance, high_performance,
                        power_saver, system_settings)</pre><hr/>
 <h1><a class="anchor" id="tools_snpe-caffe-to-dlc"></a>
snpe-caffe-to-dlc</h1>
<p>snpe-caffe-to-dlc converts a Caffe model into an SNPE DLC file.</p>
<pre class="fragment">usage: snpe-caffe-to-dlc [-h] -c CAFFE_TXT [-b CAFFE_BIN] [-d DLC]
                         [--omit_preprocessing]
                         [--encoding {argb32,rgba,nv21,bgr}]
                         [--input_size WIDTH HEIGHT]
                         [--model_version MODEL_VERSION]
                         [--disable_batchnorm_folding]
                         [--in_layer INPUT_LAYERS]
                         [--in_type {default,image,opaque}]
                         [--validation_target RUNTIME_TARGET PROCESSOR_TARGET]
                         [--strict] [--verbose]

Script to convert caffe protobuf configuration into a DLC file.

required arguments:
  -c CAFFE_TXT, --caffe_txt CAFFE_TXT
                        Input caffe proto txt configuration file

optional arguments:
  -b CAFFE_BIN, --caffe_bin CAFFE_BIN
                        Input caffe binary file containing the weight data
  -d DLC, --dlc DLC     Output DLC file containing the model. If not
                        specified, the data will be written to a file with
                        same name as the caffetxt file with a .dlc extension
  --omit_preprocessing  If specified, converter will disable preprocessing
                        specified by a data layer transform_param or any
                        preprocessing command line options
  --encoding {argb32,rgba,nv21,bgr}
                        Image encoding of the source images. Default is bgr if
                        not specified
  --input_size WIDTH HEIGHT
                        Dimensions of the source images for scaling, if
                        different from the network input.
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only
                        first 64 bytes will be stored
  --disable_batchnorm_folding
                        If not specified, converter will try to fold batchnorm
                        into previous convolution layer
  --in_layer INPUT_LAYERS
                        Name of the input layer
  --in_type {default,image,opaque}
                        Type of data expected by input layer. Type is default
                        if not specified.
  --validation_target RUNTIME_TARGET PROCESSOR_TARGET
                        A combination of processor and runtime target against
                        which model will be validated.Choices for
                        RUNTIME_TARGET: {cpu, gpu, dsp}.Choices for
                        PROCESSOR_TARGET: {snapdragon_801, snapdragon_820,
                        snapdragon_835}.If not specified, will validate model
                        against {snapdragon_820, snapdragon_835} across all
                        runtime targets.
  --strict              If specified, will validate in strict mode whereby
                        model will not be produced if it violates constraints
                        of the specified validation target.If not specified,
                        will validate model in permissive mode against the
                        specified validation target.
  --verbose             Verbose printing</pre><p>Examples of using this script can be found in <a class="el" href="model_conv_caffe.html#conversion_caffe">Converting Models from Caffe to SNPE</a>.<br />
 <br />
 Additional details:<br />
 </p><ul>
<li>
<em>omit_preprocessing argument:</em> <ul>
<li>
Disables all preprocessing. If specified, all input preprocessing is bypassed. The following are bypassed: <ul>
<li>
"encoding" command line option to the converter. </li>
<li>
"input_size" command line option to the converter. </li>
<li>
Any transform_param specified in the data layer (input layer) of the prototxt. </li>
</ul>
</li>
<li>
This argument is optional. If not provided the converter will process the preprocessing command line options as well as any applicable layer transform_param encountered in the prototxt. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>encoding argument:</em> <ul>
<li>
Specifies the encoding type of input images. </li>
<li>
A preprocessing layer is added to the network to convert input images from the specified encoding to BGR, the encoding used by Caffe. </li>
<li>
The encoding preprocessing layer can be seen when using snpe-dlc-info. </li>
<li>
Allowed options are: <ul>
<li>
<b>argb32</b>: The ARGB32 format consists of 4 bytes per pixel: one byte for Red, one for Green, one for Blue and one for the alpha channel. The alpha channel is ignored. For little endian CPUs, the byte order is BGRA. For big endian CPUs, the byte order is ARGB.  </li>
<li>
<b>rgba</b>: The RGBA format consists of 4 bytes per pixel: one byte for Red, one for Green, one for Blue and one for the alpha channel. The alpha channel is ignored. The byte ordering is endian independent and is always RGBA byte order.  </li>
<li>
<b>nv21</b>: NV21 is the Android version of YUV. The Chrominance is down sampled and has a sub sampling ratio of 4:2:0. Note that this image format has 3 channels, but the U and V channels are subsampled. For every four Y pixels there is one U and one V pixel.  </li>
<li>
<b>bgr</b>: The BGR format consists of 3 bytes per pixel: one byte for Red, one for Green and one for Blue. The byte ordering is endian independent and is always BGR byte order.  </li>
</ul>
</li>
<li>
This argument is optional. If omitted then input image encoding is assumed to be BGR and no preprocessing layer is added. </li>
<li>
See <a class="el" href="input_preprocessing.html">Image Preprocessing</a> for more details. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>input_size argument:</em> <ul>
<li>
Specifies the size of the source image for scaling. </li>
<li>
A preprocessing layer is added to the network to scale input images from the specified size to the size required by the network. </li>
<li>
The preprocessing scaling layer can be seen when using snpe-dlc-info. </li>
<li>
This argument is optional. If omitted then no preprocessing scaling layer is added. </li>
<li>
See <a class="el" href="input_preprocessing.html">Image Preprocessing</a> for more details. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>disable_batchnorm_folding argument:</em> <ul>
<li>
The disable batchnorm folding argument allows the user to turn off the optimization that folds batchnorm and batchnorm + scaling layers into previous convolution layers when possible. </li>
<li>
This argument is optional. If omitted then the converter will fold batchnorm and batchnorm + scaling layers into previous convolution layers wherever possible as an optimization. When this occurs the names of the folded batchnorm and scale layers are concatenated to the convolution layer it was folded into. <ul>
<li>
For example: if batchnorm layer named 'bn' and scale layer named 'scale' are folded into a convolution layer named 'conv', the resulting dlc will show the convolution layer to be named 'conv.bn.scale'. </li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<em>in_layer argument:</em> <ul>
<li>
Specifies the name of the input layer associated to the in_type argument that follows it. </li>
<li>
This argument can be passed more than once if you want to specify the expected data type of two or more input layers. </li>
<li>
in_layer argument needs to be followed by in_type argument. </li>
<li>
This argument is optional. If omitted for a certain input layer than the expected data type will be the default. </li>
</ul>
</li>
</ul>
<ul>
<li>
<em>in_type argument:</em> <ul>
<li>
Specifies the expected data type for a certain input layer name specified by the in_layer argument preceding it. </li>
<li>
This argument can be passed more than once if you want to specify the expected data type of two or more input layers. </li>
<li>
in_type argument needs to be preceded by in_layer argument. </li>
<li>
This argument is optional. If omitted for a certain input layer than the expected data type will be the default. </li>
<li>
Allowed options are: <ul>
<li>
<b>default</b>: Specifies that the input contains floating-point values.  </li>
<li>
<b>image</b>: Specifies that the input contains floating-point values that are all integers in the range 0..255.  </li>
<li>
<b>opaque</b>: Specifies that the input contains floating-point values that should be passed to the selected runtime without modification. <br />
 For example an opaque tensor is passed directly to the DSP without quantization.  </li>
</ul>
</li>
<li>
For example: "--in_layer data --in_type image --in_layer roi --in_type opaque". </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-caffe2-to-dlc"></a>
snpe-caffe2-to-dlc</h1>
<p>snpe-caffe2-to-dlc converts a Caffe2 model into an SNPE DLC file.</p>
<pre class="fragment">usage: snpe-caffe2-to-dlc [-h] -p PREDICT_NET -e EXEC_NET -i INPUT_DIM
                          INPUT_DIM [-d DLC] [--enable_preprocessing]
                          [--encoding {argb32,rgba,nv21,bgr}]
                          [--opaque_input [OPAQUE_INPUT [OPAQUE_INPUT ...]]]
                          [--model_version MODEL_VERSION]
                          [--reorder_list REORDER_LIST [REORDER_LIST ...]]
                          [--verbose]

Script to convert caffe2 networks into a DLC file.

optional arguments:
  -h, --help            show this help message and exit

required arguments:
  -p PREDICT_NET, --predict_net PREDICT_NET
                        Input caffe2 binary network definition protobuf
  -e EXEC_NET, --exec_net EXEC_NET
                        Input caffe2 binary file containing the weight data
  -i INPUT_DIM INPUT_DIM, --input_dim INPUT_DIM INPUT_DIM
                        The names and dimensions of the network input layers
                        specified in the format "input_name" B,C,H,W. Ex "data"
                        1,3,224,224. Note that the quotes should always be
                        included in order to handle special characters,
                        spaces, etc. For multiple inputs specify multiple
                        --input_dim on the command line like: --input_dim
                        "data1" 1,3,224,224 --input_dim "data2" 1,3,50,100 We
                        currently assume that all inputs have 4 dimensions.

optional arguments:
  -d DLC, --dlc DLC     Output DLC file containing the model. If not
                        specified, the data will be written to a file with
                        same name and location as the predict_net file with a
                        .dlc extension
  --enable_preprocessing
                        If specified, the converter will enable image mean
                        subtraction and cropping specified by ImageInputOp. Do
                        NOT enable if there is not a ImageInputOp present in
                        the Caffe2 network.
  --encoding {argb32,rgba,nv21,bgr}
                        Image encoding of the source images. Default is bgr if
                        not specified
  --opaque_input [OPAQUE_INPUT [OPAQUE_INPUT ...]]
                        A space separated list of input blob names which
                        should be treated as opaque (non-image) data. These
                        inputs will be consumed as-is by SNPE. Any input blob
                        not listed will be assumed to be image data.
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only
                        first 64 bytes will be stored
  --reorder_list REORDER_LIST [REORDER_LIST ...]
                        A list of external inputs or outputs that SNPE should
                        automatically reorder to match the specified Caffe2
                        channel ordering. Note that this feature is only
                        enabled for the GPU runtime.
  --verbose             Verbose printing</pre><hr/>
 <h1><a class="anchor" id="tools_snpe-diagview"></a>
snpe-diagview</h1>
<p>snpe-diagview loads a DiagLog file generated by snpe-net-run whenever it operates on input tensor data. The DiagLog file contains timing information information for each layer as well as the entire forward propagate time. If the run uses an input list of input tensors, the timing info reported by snpe-diagview is an average over the entire input set.</p>
<p>The snpe-net-run generates a file called "SNPEDiag_0.log", "SNPEDiag_1.log" ... , "SNPEDiag_n.log", where n corresponds to the nth iteration of the snpe-net-run execution.</p>
<pre class="fragment">usage: snpe-diagview --input_log DIAG_LOG [-h] [--output CSV_FILE]

Reads a diagnostic log and output the contents to stdout

required arguments:
  --input_log     DIAG_LOG
                Diagnostic log file (required)
optional arguments:
  --output        CSV_FILE
                Output CSV file with all diagnostic data (optional)</pre><p> <br />
</p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-dlc-info"></a>
snpe-dlc-info</h1>
<p>snpe-dlc-info outputs layer information from a DLC file, which provides information about the network model.</p>
<pre class="fragment">usage: snpe-dlc-info [-h] -i INPUT_DLC

required arguments:
  -i INPUT_DLC, --input_dlc INPUT_DLC
                        path to a DLC file</pre><p> <br />
</p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-dlc-quantize"></a>
snpe-dlc-quantize</h1>
<p>snpe-dlc-quantize converts non-quantized DLC models into quantized DLC models.</p>
<pre class="fragment">Command Line Options:
  [ -h,  --help ]       Displays this help message.
  [ --version ]         Displays version information.
  [ --verbose ]         Enable verbose user messages.
  [ --quiet ]           Disables some user messages.
  [ --silent ]          Disables all but fatal user messages.
  [ --debug=&lt;val&gt; ]     Sets the debug log level.
  [ --debug1 ]          Enables level 1 debug messages.
  [ --debug2 ]          Enables level 2 debug messages.
  [ --debug3 ]          Enables level 3 debug messages.
  [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: ".*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3"
  [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
  [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
  [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
  --input_dlc=&lt;val&gt;     Path to the dlc container containing the model for which fixed-point encoding
                        metadata should be generated. This argument is required.
  --input_list=&lt;val&gt;    Path to a file specifying the trial inputs. This file should be a plain text file,
                        containing one or more absolute file paths per line. These files will be taken to constitute
                        the trial set. Each path is expected to point to a binary file containing one trial input
                        in the 'raw' format, ready to be consumed by SNPE without any further modifications.
                        This is similar to how input is provided to snpe-net-run application.
  [ --no_weight_quantization ]
                        Generate and add the fixed-point encoding metadata but keep the weights in
                        floating point. This argument is optional.
  [ --output_dlc=&lt;val&gt; ]
                        Path at which the metadata-included quantized model container should be written.
                        If this argument is omitted, the quantized model will be written at &lt;unquantized_model_name&gt;_quantized.dlc.
  [ --use_enhanced_quantizer ]
                        Use the enhanced quantizer feature when quantizing the model.  Regular quantization determines the range using the actual
                        values of min and max of the data being quantized.  Enhanced quantization uses an algorithm to determine optimal range.  It can be
                        useful for quantizing models that have long tails in the distribution of the data being quantized.

Description:
Generate 8 bit TensorFlow style fixed point weight and activations encodings for a floating point SNPE model.</pre><p><br />
 Additional details: <br />
 </p><ul>
<li>
For specifying input_list, refer to <a class="el" href="tools.html#tools_snpe-net-run_input_list">input_list argument</a> in <a class="el" href="tools.html#tools_snpe-net-run">snpe-net-run</a> for supported input formats (in order to calculate output activation encoding information for all layers, <b>do not</b> include the line which specifies desired outputs). </li>
<li>
The tool requires the batch dimension of the DLC input file to be set to 1 during the original model conversion step. </li>
<li>
An example of quantization using snpe-dlc-quantize can be found in the C++ Tutorial section:<a class="el" href="tutorial_inceptionv3.html">Running the Inception v3 Model</a>. For details on quantization see <a class="el" href="quantized_models.html">Quantized vs Non-Quantized Models</a>. </li>
</ul>
<p><br />
</p>
<hr/>
 <h1><a class="anchor" id="tools_snpe-tensorflow-to-dlc"></a>
snpe-tensorflow-to-dlc</h1>
<p>snpe-tensorflow-to-dlc converts a TensorFlow model into an SNPE DLC file.</p>
<pre class="fragment">usage: snpe-tensorflow-to-dlc [-h] --graph GRAPH -i INPUT_NAME INPUT_DIM
                              --out_node OUT_NODE [--dlc DLC]
                              [--model_version MODEL_VERSION]
                              [--in_type {default,image}]
                              [--allow_unconsumed_nodes] [--verbose]

Script to convert a TensorFlow graph into a DLC file.

required arguments:
  --graph GRAPH         Path to TensorFlow graph def (.pb saved as binary) or
                        graph meta (.meta) file.
  -i INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers
                        specified in the format "input_name" comma-separated-
                        dimensions, for example: "data" 1,224,224,3. Note that
                        the quotes should always be included in order to handle
                        special characters, spaces, etc. For multiple inputs
                        specify multiple --input_dim on the command line like:
                        --input_dim "data1" 1,224,224,3 --input_dim "data2" 1,50,100,3.
  --out_node OUT_NODE   Name of the graph's output node.

optional arguments:
  --dlc DLC             Path to DLC file to be generated.
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only
                        first 64 bytes will be stored
  --in_type {default,image}
                        Type of data expected by input layer. Type is default
                        if not specified.
  --allow_unconsumed_nodes
                        Uses a relaxed graph node to layer mapping algorithm
                        which may not use all graph nodes during conversion
                        while retaining structural integrity.
  --verbose             Verbose printing</pre><p>Examples of using this script can be found in Converting Models from TensorFlow to SNPE.<br />
 <br />
 Additional details:<br />
 </p><ul>
<li>
<em>graph argument:</em> <ul>
<li>
The converter supports either a single frozen graph .pb file or a pair of graph meta and checkpoint files. </li>
<li>
If you are using the <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html#Saver">TensorFlow Saver</a> to save your graph during training, 3 files will be generated as described below: <ol>
<li>
&lt;model-name&gt;.meta </li>
<li>
&lt;model-name&gt; </li>
<li>
checkpoint </li>
</ol>
</li>
<li>
The converter --graph option specifies the path to the graph meta file. The converter will also use the checkpoint file to read the graph nodes parameters during conversion. The checkpoint file must have the same name without the .meta suffix. </li>
</ul>
</li>
<li>
<em>input_dim argument:</em> <ul>
<li>
Specifies the input dimensions of the graph's input node. The converter expects a comma separated list with the model's input dimensions.. </li>
<li>
<b>Multiple Inputs</b> <ul>
<li>
Networks with multiple inputs must provide both --<b>in_node</b> and --<b>input_dim</b>, one for each input node. </li>
</ul>
</li>
</ul>
</li>
<li>
<em>in_node argument:</em> <ul>
<li>
Specifies the input node name. The converter requires a node name as input from which it will create an input layer by using the node output tensor dimensions. When defining a graph, there is typically a placeholder name used as input during training in the graph. The placeholder tensor name is the name you must use as the argument. It is also possible to use other types of nodes as input, however the node used as input will not be used as part of a layer other than the input layer. </li>
<li>
<b>Multiple Inputs</b> <ul>
<li>
Networks with multiple inputs must provide both --<b>in_node</b> and --<b>input_dim</b>, one for each input node. </li>
</ul>
</li>
</ul>
</li>
<li>
<em>out_node argument:</em> <ul>
<li>
<p class="startli">The name of the last node in your TensorFlow graph which will represent the output layer of your network.</p>
<p class="endli"></p>
</li>
<li>
<b>Multiple Outputs</b> <ul>
<li>
Networks with multiple outputs must provide several --<b>out_node</b> arguments, one for each output node. </li>
</ul>
</li>
</ul>
</li>
<li>
<em>dlc argument:</em> <ul>
<li>
Specifies the output DLC file name. </li>
<li>
This argument is optional. If not provided the converter will create a DLC file file with the same name as the graph file name, with a .dlc file extension. </li>
</ul>
</li>
</ul>
<hr/>
 <h1><a class="anchor" id="tools_snpe-onnx-to-dlc"></a>
snpe-onnx-to-dlc</h1>
<p>snpe-onnx-to-dlc converts a serialized ONNX model into a SNPE DLC file.</p>
<pre class="fragment">usage: snpe-onnx-to-dlc [-h] --model_path MODEL_PATH [--dlc_path DLC_PATH]
                        [--encoding ENCODING ENCODING] [--debug]

optional arguments:
  -h, --help            show this help message and exit
  --model_path MODEL_PATH, -m MODEL_PATH
                        Path to the source ONNX model.
  --dlc_path DLC_PATH, -d DLC_PATH
                        Path where the converted DLC model should be saved.
  --encoding ENCODING ENCODING
                        Set the image encoding for an input buffer. This
                        should be specifed in the format "--encoding &lt;input
                        name&gt; &lt;encoding&gt;", where encoding is one of: "argb32",
                        "rgba", "nv21", "opaque", or "bgr". The default
                        encoding for all inputs not so described is "bgr".
                        "opaque" inputs will be interpreted as-is by SNPE, and
                        not subject to order transformations.
  --debug               Run the converter in debug mode.</pre><p>For more information, see <a class="el" href="model_conv_onnx.html">ONNX Model Conversion</a> </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016 Qualcomm Technologies, Inc. All rights reserved. 
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 08/23/16   LB   Created to support generation of SNPE UG and SDK HTML document.
 footer.html 
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
		<p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
		</p>
		</li>
  </ul>
</div>
</body>
</html>
